{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare ARGO\n",
    "\n",
    "Converts ARGO netcdfs (previously downloaded through FTP) to Feather storing only the required variables and adding additional data to compare with models in the future. Additional data is CT and depth information computed using gsw package. ARGO data can be stored in daily or monthly files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files for date 20000326:\n",
      "Processing files for date 20000214:\n",
      "Processing files for date 20000811:\n",
      "Processing files for date 20000517:\n",
      "Processing files for date 20000125:\n",
      "Processing files for date 20000713:\n",
      "Processing files for date 20000803:\n",
      "Processing files for date 20000405:\n",
      "Processing files for date 20001010:\n",
      "Processing files for date 20000406:\n",
      "Processing files for date 20001030:\n",
      "Processing files for date 20000217:\n",
      "Processing files for date 20000615:\n",
      "Processing files for date 20001210:\n",
      "Processing files for date 20000510:\n",
      "Processing files for date 20000427:\n",
      "Processing files for date 20001111:\n",
      "Processing files for date 20000126:\n",
      "Processing files for date 20000127:\n",
      "Processing files for date 20000216:\n",
      "Processing files for date 20000528:\n",
      "Processing files for date 20001204:\n",
      "Processing files for date 20000722:\n",
      "Processing files for date 20001114:\n",
      "Processing files for date 20000613:\n",
      "Processing files for date 20000402:\n",
      "Processing files for date 20000120:\n",
      "Processing files for date 20000401:\n",
      "Processing files for date 20001220:\n",
      "Processing files for date 20001009:\n",
      "Processing files for date 20000106:\n",
      "Processing files for date 20001022:\n",
      "Processing files for date 20000623:\n",
      "Processing files for date 20000206:\n",
      "Processing files for date 20000423:\n",
      "Processing files for date 20000309:\n",
      "Processing files for date 20000714:\n",
      "Processing files for date 20000417:\n",
      "Processing files for date 20000119:\n",
      "Processing files for date 20000919:\n",
      "Processing files for date 20001012:\n",
      "Processing files for date 20000720:\n",
      "Processing files for date 20000916:\n",
      "Processing files for date 20000810:\n",
      "Processing files for date 20000103:\n",
      "Processing files for date 20001031:\n",
      "Processing files for date 20000922:\n",
      "Processing files for date 20000514:\n",
      "Processing files for date 20000109:\n",
      "Processing files for date 20000717:\n",
      "Processing files for date 20001215:\n",
      "Processing files for date 20001106:\n",
      "Processing files for date 20001224:\n",
      "Processing files for date 20001203:\n",
      "Processing files for date 20000112:\n",
      "Processing files for date 20000122:\n",
      "Processing files for date 20001019:\n",
      "Processing files for date 20000719:\n",
      "Processing files for date 20000520:\n",
      "Processing files for date 20000124:\n",
      "Processing files for date 20001122:\n",
      "Processing files for date 20000718:\n",
      "Processing files for date 20001023:\n",
      "Processing files for date 20001214:\n",
      "Processing files for date 20001102:\n",
      "Processing files for date 20001129:\n",
      "Processing files for date 20000927:\n",
      "Processing files for date 20000123:\n",
      "Processing files for date 20000727:\n",
      "Processing files for date 20000202:\n",
      "Processing files for date 20000602:\n",
      "Processing files for date 20001104:\n",
      "Processing files for date 20000310:\n",
      "Processing files for date 20000207:\n",
      "Processing files for date 20001109:\n",
      "Processing files for date 20001115:\n",
      "Processing files for date 20000918:\n",
      "Processing files for date 20000701:\n",
      "Processing files for date 20000227:\n",
      "Processing files for date 20000320:\n",
      "Processing files for date 20000914:\n",
      "Processing files for date 20001004:\n",
      "Processing files for date 20000418:\n",
      "Processing files for date 20000913:\n",
      "Processing files for date 20000426:\n",
      "Processing files for date 20000507:\n",
      "Processing files for date 20001223:\n",
      "Processing files for date 20000830:\n",
      "Processing files for date 20000920:\n",
      "Processing files for date 20000816:\n",
      "Processing files for date 20000215:\n",
      "Processing files for date 20001219:\n",
      "Processing files for date 20000925:\n",
      "Processing files for date 20000627:\n",
      "Processing files for date 20000924:\n",
      "Processing files for date 20000104:\n",
      "Processing files for date 20000329:\n",
      "Processing files for date 20000121:\n",
      "Processing files for date 20000921:\n",
      "Processing files for date 20000805:\n",
      "Processing files for date 20000530:\n",
      "Processing files for date 20000203:\n",
      "Processing files for date 20000419:\n",
      "Processing files for date 20000318:\n",
      "Processing files for date 20000128:\n",
      "Processing files for date 20001118:\n",
      "Processing files for date 20000511:\n",
      "Processing files for date 20000113:\n",
      "Processing files for date 20001127:\n",
      "Processing files for date 20001006:\n",
      "Processing files for date 20000827:\n",
      "Processing files for date 20000428:\n",
      "Processing files for date 20000201:\n",
      "Processing files for date 20001216:\n",
      "Processing files for date 20001007:\n",
      "Processing files for date 20000314:\n",
      "Processing files for date 20001024:\n",
      "Processing files for date 20001208:\n",
      "Processing files for date 20000712:\n",
      "Processing files for date 20000519:\n",
      "Processing files for date 20001205:\n",
      "Processing files for date 20001025:\n",
      "Processing files for date 20000213:\n",
      "Processing files for date 20000313:\n",
      "Processing files for date 20000726:\n",
      "Processing files for date 20001222:\n",
      "Processing files for date 20001107:\n",
      "Processing files for date 20000324:\n",
      "Processing files for date 20000509:\n",
      "Processing files for date 20000205:\n",
      "Processing files for date 20000708:\n",
      "Processing files for date 20000605:\n",
      "Processing files for date 20000505:\n",
      "Processing files for date 20001227:\n",
      "Processing files for date 20001130:\n",
      "Processing files for date 20000220:\n",
      "Processing files for date 20000506:\n",
      "Processing files for date 20000518:\n",
      "Processing files for date 20001002:\n",
      "Processing files for date 20000212:\n",
      "Processing files for date 20000319:\n",
      "Processing files for date 20000430:\n",
      "Processing files for date 20000804:\n",
      "Processing files for date 20000622:\n",
      "Processing files for date 20001015:\n",
      "Processing files for date 20000910:\n",
      "Processing files for date 20000628:\n",
      "Processing files for date 20001231:\n",
      "Processing files for date 20001112:\n",
      "Processing files for date 20001125:\n",
      "Processing files for date 20000404:\n",
      "Processing files for date 20000815:\n",
      "Processing files for date 20001020:\n",
      "Processing files for date 20000909:\n",
      "Processing files for date 20000107:\n",
      "Processing files for date 20000721:\n",
      "Processing files for date 20001225:\n",
      "Processing files for date 20000303:\n",
      "Processing files for date 20000416:\n",
      "Processing files for date 20000813:\n",
      "Processing files for date 20000114:\n",
      "Processing files for date 20001110:\n",
      "Processing files for date 20001108:\n",
      "Processing files for date 20000905:\n",
      "Processing files for date 20000907:\n",
      "Processing files for date 20001113:\n",
      "Processing files for date 20000911:\n",
      "Processing files for date 20001101:\n",
      "Processing files for date 20000725:\n",
      "Processing files for date 20000612:\n",
      "Processing files for date 20000626:\n",
      "Processing files for date 20001209:\n",
      "Processing files for date 20000818:\n",
      "Processing files for date 20000322:\n",
      "Processing files for date 20000710:\n",
      "Processing files for date 20000923:\n",
      "Processing files for date 20001018:\n",
      "Processing files for date 20000814:\n",
      "Processing files for date 20000529:\n",
      "Processing files for date 20001116:\n",
      "Processing files for date 20000611:\n",
      "Processing files for date 20000625:\n",
      "Processing files for date 20000321:\n",
      "Processing files for date 20000424:\n",
      "Processing files for date 20000716:\n",
      "Processing files for date 20000525:\n",
      "Processing files for date 20000117:\n",
      "Processing files for date 20000620:\n",
      "Processing files for date 20000704:\n",
      "Processing files for date 20000225:\n",
      "Processing files for date 20000729:\n",
      "Processing files for date 20000409:\n",
      "Processing files for date 20000523:\n",
      "Processing files for date 20001217:\n",
      "Processing files for date 20000422:\n",
      "Processing files for date 20000501:\n",
      "Processing files for date 20000312:\n",
      "Processing files for date 20000711:\n",
      "Processing files for date 20001226:\n",
      "Processing files for date 20000703:\n",
      "Processing files for date 20000421:\n",
      "Processing files for date 20000723:\n",
      "Processing files for date 20001001:\n",
      "Processing files for date 20000606:\n",
      "Processing files for date 20000512:\n",
      "Processing files for date 20000330:\n",
      "Processing files for date 20000317:\n",
      "Processing files for date 20000728:\n",
      "Processing files for date 20000730:\n",
      "Processing files for date 20000328:\n",
      "Processing files for date 20000504:\n",
      "Processing files for date 20001119:\n",
      "Processing files for date 20000817:\n",
      "Processing files for date 20000826:\n",
      "Processing files for date 20000926:\n",
      "Processing files for date 20000211:\n",
      "Processing files for date 20000930:\n",
      "Processing files for date 20000929:\n",
      "Processing files for date 20000408:\n",
      "Processing files for date 20000822:\n",
      "Processing files for date 20001128:\n",
      "Processing files for date 20001229:\n",
      "Processing files for date 20000508:\n",
      "Processing files for date 20000707:\n",
      "Processing files for date 20000824:\n",
      "Processing files for date 20000411:\n",
      "Processing files for date 20001021:\n",
      "Processing files for date 20000630:\n",
      "Processing files for date 20000522:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files for date 20000603:\n",
      "Processing files for date 20000223:\n",
      "Processing files for date 20000709:\n",
      "Processing files for date 20001121:\n",
      "Processing files for date 20000410:\n",
      "Processing files for date 20000715:\n",
      "Processing files for date 20000802:\n",
      "Processing files for date 20000903:\n",
      "Processing files for date 20000526:\n",
      "Processing files for date 20000305:\n",
      "Processing files for date 20000208:\n",
      "Processing files for date 20000516:\n",
      "Processing files for date 20000129:\n",
      "Processing files for date 20000222:\n",
      "Processing files for date 20000218:\n",
      "Processing files for date 20001017:\n",
      "Processing files for date 20001028:\n",
      "Processing files for date 20000130:\n",
      "Processing files for date 20001027:\n",
      "Processing files for date 20000908:\n",
      "Processing files for date 20000325:\n",
      "Processing files for date 20000302:\n",
      "Processing files for date 20000311:\n",
      "Processing files for date 20000901:\n",
      "Processing files for date 20000706:\n",
      "Processing files for date 20000115:\n",
      "Processing files for date 20001005:\n",
      "Processing files for date 20000301:\n",
      "Processing files for date 20000821:\n",
      "Processing files for date 20000502:\n",
      "Processing files for date 20001123:\n",
      "Processing files for date 20001120:\n",
      "Processing files for date 20000221:\n",
      "Processing files for date 20000917:\n",
      "Processing files for date 20000515:\n",
      "Processing files for date 20001202:\n",
      "Processing files for date 20000819:\n",
      "Processing files for date 20000806:\n",
      "Processing files for date 20000307:\n",
      "Processing files for date 20000323:\n",
      "Processing files for date 20001103:\n",
      "Processing files for date 20000414:\n",
      "Processing files for date 20000705:\n",
      "Processing files for date 20000116:\n",
      "Processing files for date 20001230:\n",
      "Processing files for date 20000413:\n",
      "Processing files for date 20000415:\n",
      "Processing files for date 20001117:\n",
      "Processing files for date 20000101:\n",
      "Processing files for date 20000915:\n",
      "Processing files for date 20001124:\n",
      "Processing files for date 20000403:\n",
      "Processing files for date 20000617:\n",
      "Processing files for date 20000624:\n",
      "Processing files for date 20000904:\n",
      "Processing files for date 20000614:\n",
      "Processing files for date 20000829:\n",
      "Processing files for date 20001211:\n",
      "Processing files for date 20000601:\n",
      "Processing files for date 20001126:\n",
      "Processing files for date 20000825:\n",
      "Processing files for date 20000425:\n",
      "Processing files for date 20001207:\n",
      "Processing files for date 20000906:\n",
      "Processing files for date 20000531:\n",
      "Processing files for date 20000228:\n",
      "Processing files for date 20000812:\n",
      "Processing files for date 20000131:\n",
      "Processing files for date 20000315:\n",
      "Processing files for date 20000118:\n",
      "Processing files for date 20000110:\n",
      "Processing files for date 20000224:\n",
      "Processing files for date 20001218:\n",
      "Processing files for date 20000820:\n",
      "Processing files for date 20000828:\n",
      "Processing files for date 20001016:\n",
      "Processing files for date 20000801:\n",
      "Processing files for date 20000105:\n",
      "Processing files for date 20001003:\n",
      "Processing files for date 20000616:\n",
      "Processing files for date 20000928:\n",
      "Processing files for date 20000807:\n",
      "Processing files for date 20001013:\n",
      "Processing files for date 20000609:\n",
      "Processing files for date 20000808:\n",
      "Processing files for date 20000108:\n",
      "Processing files for date 20000619:\n",
      "Processing files for date 20000412:\n",
      "Processing files for date 20000610:\n",
      "Processing files for date 20001011:\n",
      "Processing files for date 20001008:\n",
      "Processing files for date 20000219:\n",
      "Processing files for date 20000524:\n",
      "Processing files for date 20000102:\n",
      "Processing files for date 20000621:\n",
      "Processing files for date 20000527:\n",
      "Processing files for date 20000809:\n",
      "Processing files for date 20000226:\n",
      "Processing files for date 20000607:\n",
      "Processing files for date 20000229:\n",
      "Processing files for date 20000912:\n",
      "Processing files for date 20001228:\n",
      "Processing files for date 20000306:\n",
      "Processing files for date 20000608:\n",
      "Processing files for date 20001206:\n",
      "Processing files for date 20000521:\n",
      "Processing files for date 20000902:\n",
      "Processing files for date 20001014:\n",
      "Processing files for date 20001026:\n",
      "Processing files for date 20000210:\n",
      "Processing files for date 20000331:\n",
      "Processing files for date 20000209:\n",
      "Processing files for date 20000316:\n",
      "Processing files for date 20000702:\n",
      "Processing files for date 20000823:\n",
      "Processing files for date 20000618:\n",
      "Processing files for date 20000420:\n",
      "Processing files for date 20000831:\n",
      "Processing files for date 20000407:\n",
      "Processing files for date 20000629:\n",
      "Processing files for date 20001213:\n",
      "Processing files for date 20001221:\n",
      "Processing files for date 20000604:\n",
      "Processing files for date 20000513:\n",
      "Processing files for date 20000308:\n",
      "Processing files for date 20001105:\n",
      "Processing files for date 20000327:\n",
      "Processing files for date 20001029:\n",
      "Processing files for date 20001212:\n",
      "Processing files for date 20000111:\n",
      "Processing files for date 20000724:\n",
      "Processing files for date 20000731:\n",
      "Processing files for date 20000429:\n",
      "Processing files for date 20001201:\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import xarray as xr\n",
    "import os\n",
    "import gsw\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "root_folder = 'db'\n",
    "outdir = 'argos_collocated'\n",
    "coords = 'coords025.nc'\n",
    "year = '2000'\n",
    "\n",
    "def format_date(row):\n",
    "    \"\"\"\n",
    "    Converts the Julian Date column to datetime\n",
    "    \"\"\"\n",
    "    return row['JULD'].strftime(\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "\n",
    "def compute_AS(row):\n",
    "    return gsw.SA_from_SP(row['PSAL_ADJUSTED'], row['PRES_ADJUSTED'], row['LONGITUDE'], row['LATITUDE'])\n",
    "\n",
    "\n",
    "def compute_CT(row):\n",
    "    \"\"\"\n",
    "    Computes Conservative temperature from In-situ temperature, absolute salinity and pressure\n",
    "    \"\"\"\n",
    "    return gsw.CT_from_t(row['ASAL'], row['TEMP_ADJUSTED'], row['PRES_ADJUSTED'])\n",
    "\n",
    "\n",
    "def compute_Z(row):\n",
    "    \"\"\"\n",
    "    Computes Height from sea pressure and latitude\n",
    "    \"\"\"\n",
    "    return gsw.z_from_p(row['PRES_ADJUSTED'], row['LATITUDE'], 0, 0)\n",
    "\n",
    "\n",
    "def process_folder(folder_path, variables, outdir, name_offset, coords):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        process_argo_profile(filename,\n",
    "                             variables, coords)\n",
    "\n",
    "\n",
    "def process_argo_profile(filename, variables, coords):\n",
    "\n",
    "    try:\n",
    "        ds = xr.open_dataset(filename)\n",
    "        coordinates = xr.open_dataset(coords)\n",
    "        # subset a les variables que me interesan\n",
    "        ds = ds[variables]\n",
    "        df = ds.to_dataframe().reset_index()\n",
    "        df = df.drop(['N_PROF', 'N_LEVELS'], axis=1)\n",
    "        df = df.dropna(axis=0)\n",
    "        \n",
    "        if not df.empty and df['PRES_ADJUSTED'].max() > 700:\n",
    "            # Convierto las variables guardadas como 'bytes' a string o a int\n",
    "            df['PSAL_ADJUSTED_QC'] = df['PSAL_ADJUSTED_QC'].str.decode(\n",
    "                'utf-8').astype(int)\n",
    "            df['TEMP_ADJUSTED_QC'] = df['TEMP_ADJUSTED_QC'].str.decode(\n",
    "                'utf-8').astype(int)\n",
    "            df['PRES_ADJUSTED_QC'] = df['PRES_ADJUSTED_QC'].str.decode(\n",
    "                'utf-8').astype(int)\n",
    "\n",
    "            df['DIRECTION'] = df['DIRECTION'].str.decode('utf-8')\n",
    "\n",
    "            df = df[df['PSAL_ADJUSTED_QC'] == 1]\n",
    "            if (df.empty):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = df[df['TEMP_ADJUSTED_QC'] == 1]\n",
    "            if (df.empty):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = df[df['PRES_ADJUSTED_QC'] == 1]\n",
    "            if (df.empty):\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Apply the custom formatting function to the 'JULD' column\n",
    "            df['JULD'] = df.apply(format_date, axis=1)\n",
    "            \n",
    "            # Compute AS (absolute salinity)\n",
    "            df['ASAL'] = df.apply(compute_AS, axis=1)\n",
    "            # Compute CT\n",
    "            df['CTEMP'] = df.apply(compute_CT, axis=1)\n",
    "            # Compute Z\n",
    "            df['HEIGHT'] = df.apply(compute_Z, axis=1)\n",
    "            df = df.sort_values(by='HEIGHT')\n",
    "            \n",
    "            # Remove ghost measures, keep only one\n",
    "            df['spacing_diff'] = df['HEIGHT'].diff()\n",
    "            \n",
    "            threshold = 1.5 # miden cada 2m, a veces mas. Cuando hay ghosts, lo hacen sobre 20cm\n",
    "            \n",
    "            mask = (df['spacing_diff'] >= threshold) & ~df['HEIGHT'].isna()\n",
    "            df[df['spacing_diff'].isna()]['HEIGHT'] = 2\n",
    "            filtered_df = df[df['spacing_diff'] >= threshold]\n",
    "      \n",
    "            df = filtered_df.drop(columns=['spacing_diff'])\n",
    "            \n",
    "            \n",
    "\n",
    "            # latitude bining in the CMEMS grid\n",
    "            lat = coordinates['latitude'].values\n",
    "            digitized = np.digitize(df['LATITUDE'], lat, right=True)\n",
    "            df['LATITUDE_MODEL'] = [lat[i - 1] if i > 0 else lat[0]\n",
    "                              for i in digitized]\n",
    "            df['ILAT_MODEL'] = digitized\n",
    "\n",
    "            # longitude bining in the CMEMS grid\n",
    "            lon = coordinates['longitude'].values\n",
    "            digitized = np.digitize(df['LONGITUDE'], lon, right=True)\n",
    "            df['LONGITUDE_MODEL'] = [lon[i - 1] if i > 0 else lon[0]\n",
    "                               for i in digitized]\n",
    "            df['ILON_MODEL'] = digitized\n",
    "\n",
    "            # longitude bining in the CMEMS grid\n",
    "            depth = coordinates['depth'].values * -1\n",
    "            digitized = np.digitize(df['HEIGHT'], depth, right=True)\n",
    "            df['HEIGHT_MODEL'] = [depth[i - 1] if i > 0 else depth[0] for i in digitized]\n",
    "            df['IHEIGHT_MODEL'] = digitized\n",
    "\n",
    "            \n",
    "            file = os.path.splitext(filename)[0]\n",
    "            df['ORIGINAL'] = file\n",
    "            df = df.drop(['POSITION_QC', 'PSAL_ADJUSTED', 'PSAL_ADJUSTED_QC', 'TEMP_ADJUSTED',\n",
    "                         'TEMP_ADJUSTED_QC', 'PRES_ADJUSTED', 'PRES_ADJUSTED_QC'], axis=1)\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except Exception as error:\n",
    "        print(\"err: \", error, \"in file \", filename)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to process files for a given date\n",
    "def process_files(outdir, date, files, coords):\n",
    "    # List of variables that we want to store from ARGO files\n",
    "    variables = [\n",
    "        'LATITUDE',\n",
    "        'LONGITUDE',\n",
    "        'POSITION_QC',\n",
    "        'PSAL_ADJUSTED',\n",
    "        'PSAL_ADJUSTED_QC',\n",
    "        'TEMP_ADJUSTED',\n",
    "        'TEMP_ADJUSTED_QC',\n",
    "        'PRES_ADJUSTED',\n",
    "        'PRES_ADJUSTED_QC',\n",
    "        'DIRECTION',\n",
    "        'JULD'\n",
    "    ]\n",
    "\n",
    "    print(f\"Processing files for date {date}:\")\n",
    "    datasets = []\n",
    "    for file_path in files:\n",
    "        dataset = process_argo_profile(file_path, variables, coords)\n",
    "        if len(dataset.columns) > 0:\n",
    "            datasets.append(dataset)\n",
    "\n",
    "    if (len(datasets) > 0):\n",
    "        daily_ds = pd.concat(datasets, axis=0).reset_index()\n",
    "        daily_ds.to_feather(f'{outdir}/{date}.feather')\n",
    "\n",
    "\n",
    "def get_unique_dates(root_folder):\n",
    "    \"\"\"\n",
    "    Returns a list of all unique dates in the directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "        root_folder (str): The root folder path.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of unique dates.\n",
    "    \"\"\"\n",
    "    unique_dates = set()\n",
    "\n",
    "    for provider_folder in os.listdir(root_folder):\n",
    "        provider_path = os.path.join(root_folder, provider_folder)\n",
    "\n",
    "        if os.path.isdir(provider_path):\n",
    "            for date_folder in os.listdir(provider_path):\n",
    "                unique_dates.add(date_folder)\n",
    "\n",
    "    return list(unique_dates)\n",
    "\n",
    "def get_files_for_date(root_folder, target_date):\n",
    "    \"\"\"\n",
    "    Returns a list of all files for a given date in the directory structure.\n",
    "    \n",
    "    Parameters:\n",
    "        root_folder (str): The root folder path.\n",
    "        target_date (str): The date for which to retrieve files.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of file paths for the given date.\n",
    "    \"\"\"\n",
    "    files_for_date = []\n",
    "\n",
    "    for provider_folder in os.listdir(root_folder):\n",
    "        provider_path = os.path.join(root_folder, provider_folder)\n",
    "\n",
    "        if os.path.isdir(provider_path):\n",
    "            date_folder_path = os.path.join(provider_path, target_date)\n",
    "\n",
    "            if os.path.isdir(date_folder_path):\n",
    "                files_for_date.extend([os.path.join(date_folder_path, file) for file in os.listdir(date_folder_path)])\n",
    "\n",
    "    return files_for_date\n",
    "\n",
    "dates = get_unique_dates(root_folder)\n",
    "dates_year = [s for s in dates if s.startswith(year)]\n",
    "for date in dates_year:\n",
    "    process_files(outdir, date, get_files_for_date(root_folder, date), coords) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
