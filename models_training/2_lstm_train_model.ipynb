{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "489a4a52-331b-4de9-b7d4-5f483583dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 14:47:17.021248: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-08 14:47:17.054207: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-08 14:47:17.054237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-08 14:47:17.055300: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-08 14:47:17.061161: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-08 14:47:17.758663: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\"\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from subprocess import call\n",
    "import warnings\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")  # specify to ignore warning messages\n",
    "from keras.optimizers import SGD\n",
    "from numpy import hstack\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from pandas import DataFrame\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Lambda\n",
    "from keras import backend as K\n",
    "from netCDF4 import Dataset\n",
    "from netCDF4 import getlibversion\n",
    "import glob\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] =\"0\"\n",
    "#tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "arr_dir = '../../model_collocated_10d_filled/'\n",
    "\n",
    "# define model\n",
    "activ = 'tanh'#'softsign'#\n",
    "opt='Adam'\n",
    "pat=30\n",
    "n_epochs = 1000\n",
    "val_split=.15\n",
    "dropout_fraction=.2\n",
    "n_units1 =35\n",
    "n_units2 =35\n",
    "# n_units3 =n_unit\n",
    "batch_size = 16 #16\n",
    "batch_size_val = batch_size//4\n",
    "n_depth = 46\n",
    "n_var_in = 10\n",
    "n_var_out = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb72f2e-693c-4026-b1df-5d5591cbbd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 13:52:01.297798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2721 MB memory:  -> device: 0, name: NVIDIA T1000, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 46, 35)            6440      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 46, 35)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 46, 35)            9940      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 46, 35)            0         \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 46, 2)             72        \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16452 (64.27 KB)\n",
      "Trainable params: 16452 (64.27 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 13:52:01.782160: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n"
     ]
    }
   ],
   "source": [
    "def PermaDropout(rate):\n",
    "    return Lambda(lambda x: Dropout(x, rate=rate))\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=[n_depth, n_var_in], dtype = tf.float32),\n",
    "    LSTM(n_units1, return_sequences=True, activation=activ),\n",
    "    Dropout(dropout_fraction),\n",
    "    LSTM(n_units2, return_sequences=True, activation=activ),\n",
    "    Dropout(dropout_fraction),\n",
    "    TimeDistributed(Dense(n_var_out))\n",
    "    #TimeDistributed(Dense((n_depth,n_var_out)))\n",
    "])\n",
    "\n",
    "model.compile(loss='mse', optimizer=keras.optimizers.Adam(learning_rate=1e-5), metrics=['accuracy'])\n",
    "dummy_batch_X = np.zeros((batch_size, n_depth, n_var_in))  # Replace with your actual batch data\n",
    "# Call the model on the dummy batch to build the model\n",
    "model(dummy_batch_X)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b181161-caf1-432c-8a52-d83d84aa4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import json \n",
    "\n",
    "def load_data_from_feather(daily_ds):\n",
    "    if isinstance(daily_ds, bytes):\n",
    "        daily_ds = daily_ds.decode('utf-8')    \n",
    "    df = pd.read_feather('../../model_collocated_10d_filled/'+daily_ds)\n",
    "    asal_height_cols = [col for col in df.columns if col.startswith('ASAL_height')]\n",
    "    ctemp_height_cols = [col for col in df.columns if col.startswith('CTEMP_height')]\n",
    "    \n",
    "    other_vars = ['LATITUDE', 'LONGITUDE', 'SSS', 'SST', 'SSH', 'MLD', 'UO', 'VO']\n",
    "    \n",
    "    new_df = df[other_vars]\n",
    "    \n",
    "    height_dfs = []\n",
    "    for asal_col, ctemp_col in zip(asal_height_cols, ctemp_height_cols):\n",
    "        height_df = new_df.copy()\n",
    "        height_df['HEIGHT'] = asal_col.split('_height')[-1]\n",
    "        height_df['ASAL'] = df[asal_col]  \n",
    "        height_df['CTEMP'] = df[ctemp_col] \n",
    "        height_dfs.append(height_df)\n",
    "    \n",
    "    final_df = pd.concat(height_dfs, ignore_index=True)\n",
    "    final_df = final_df.dropna()\n",
    "\n",
    "    pivot_table = final_df.pivot_table(index=['LATITUDE', 'LONGITUDE', 'HEIGHT'], values=['SSS', 'SST', 'SSH', 'MLD', 'UO', 'VO', 'ASAL', 'CTEMP'])\n",
    "\n",
    "    filename = os.path.basename(daily_ds)\n",
    "    date_str = filename.split('.')[0] \n",
    "    date = pd.to_datetime(date_str, format='%Y%m%d')\n",
    "    day_of_year = date.dayofyear\n",
    "\n",
    "    pivot_table['DATE'] = day_of_year\n",
    "    pivot_table = pivot_table.reset_index()\n",
    "    pivot_table = pivot_table.astype(float)\n",
    "    pivot_table = pivot_table[['LATITUDE', 'LONGITUDE', 'HEIGHT', 'SSS', 'SST', 'SSH', 'MLD', 'UO', 'VO', 'DATE', 'ASAL', 'CTEMP']]\n",
    "\n",
    "    cols = ['LATITUDE', 'LONGITUDE', 'SSS', 'SST', 'SSH', 'MLD', 'UO', 'VO', 'DATE', 'HEIGHT']\n",
    "\n",
    "    groups = pivot_table.groupby(['LATITUDE', 'LONGITUDE'])\n",
    "    \n",
    "    arrays = []\n",
    "    \n",
    "    n_row = 46\n",
    "    \n",
    "    with open('min_max_values.json', 'r') as f:\n",
    "        min_max_values = json.load(f)\n",
    "        \n",
    "    for name, group in groups:\n",
    "        if len(group) == n_row:\n",
    "            group_sorted = group.sort_values(by='HEIGHT')\n",
    "            for column in min_max_values['min_values'].keys():\n",
    "                min_value = min_max_values['min_values'][column]\n",
    "                max_value = min_max_values['max_values'][column]\n",
    "                group_sorted[column] = (group_sorted[column] - min_value) / (max_value - min_value)\n",
    "\n",
    "            array = group_sorted.values\n",
    "            if (np.any(array)):\n",
    "                arrays.append(array)\n",
    "    \n",
    "    all_arr = np.stack(arrays)\n",
    "    X = all_arr[:,:,0:10]\n",
    "    y = all_arr[:,:,10:12]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def data_generator(feather_files, batch_size):\n",
    "    for file in feather_files:\n",
    "        X, y = load_data_from_feather(file)\n",
    "        num_samples = X.shape[0]\n",
    "        num_batches = num_samples // batch_size\n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size\n",
    "            batch_X = X[start_idx:end_idx]\n",
    "            batch_y = y[start_idx:end_idx]            \n",
    "            yield batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8e310d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_2827368/2020883380.py:24: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /tmp/ipykernel_2827368/2020883380.py:24: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 14:47:22.272206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2720 MB memory:  -> device: 0, name: NVIDIA T1000, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (16, 46, 10)\n",
      "Output shape: (16, 46, 2)\n",
      "Input shape: (4, 46, 10)\n",
      "Output shape: (4, 46, 2)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "feather_files = [f for f in os.listdir(arr_dir) if f.endswith('.feather') and os.path.isfile(os.path.join(arr_dir, f))]\n",
    "\n",
    "start_date = '20100101'\n",
    "end_date = '20221231'\n",
    "\n",
    "random.seed(56)\n",
    "random.shuffle(feather_files)\n",
    "\n",
    "# Set 0.8 training and 0.2 testing\n",
    "split_point = int(len(feather_files) * 0.8)  \n",
    "\n",
    "training_dates = feather_files[:split_point]\n",
    "testing_dates = feather_files[split_point:]\n",
    "\n",
    "split_point_val = int(len(training_dates)*0.9)\n",
    "validation_dates = training_dates[split_point_val:]\n",
    "training_dates = training_dates[:split_point_val]\n",
    "\n",
    "dataset_training = tf.data.Dataset.from_generator(\n",
    "    data_generator,\n",
    "    args=[training_dates, batch_size],  \n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, n_depth, n_var_in], [None, n_depth, n_var_out])\n",
    ")\n",
    "\n",
    "dataset_training = dataset_training.repeat()\n",
    "\n",
    "# Convert string dates to datetime objects\n",
    "dataset_validation = tf.data.Dataset.from_generator(\n",
    "    data_generator,\n",
    "    args=[validation_dates, batch_size_val],  # Pass batch_size as an argument\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([None, n_depth, n_var_in], [None, n_depth, n_var_out])\n",
    ")\n",
    "\n",
    "dataset_validation = dataset_validation.repeat()\n",
    "\n",
    "for batch_X, batch_y in dataset_training.take(1):\n",
    "    print(\"Input shape:\", batch_X.shape)\n",
    "    print(\"Output shape:\", batch_y.shape)\n",
    "    \n",
    "for batch_X, batch_y in dataset_validation.take(1):\n",
    "    print(\"Input shape:\", batch_X.shape)\n",
    "    print(\"Output shape:\", batch_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# fit model\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=pat)\n",
    "steps_per_epoch = 8000\n",
    "\n",
    "validation_steps = 2000\n",
    "\n",
    "history = model.fit(\n",
    "    dataset_training,\n",
    "    epochs=n_epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=dataset_validation,\n",
    "    validation_steps=validation_steps,\n",
    "    verbose=1, callbacks=[es])\n",
    "model.save('LSTM_512_512.h5')\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "train = history.history['loss']\n",
    "val = history.history['val_loss']\n",
    "\n",
    "# plot train and validation loss across multiple runs\n",
    "plt.plot(train, color='blue', label='train')\n",
    "plt.plot(val, color='orange', label='validation')\n",
    "plt.title('LSTM ' + str(n_units1) + '-' + str(n_units2) + ' model')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show(block=False)\n",
    "plt.savefig('LSTM_' + str(n_units1) + '_' + str(n_units2)+ '_loss.eps', dpi=150)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
